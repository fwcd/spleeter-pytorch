#!/usr/bin/env python3

# A script for converting the Spleeter separator to Core ML.

# Useful resources:
# - https://github.com/deezer/spleeter/issues/210
# - https://github.com/deezer/spleeter/issues/155
# - https://twitter.com/ExtractorVocal/status/1342643493227773952

import argparse
import coremltools as ct
import librosa
import torch

from pathlib import Path
from spleeter_pytorch.estimator import Estimator

ROOT = Path(__file__).resolve().parent

def main():
    parser = argparse.ArgumentParser(description='Converts Spleeter (minus the STFT preprocessing) to Core ML')
    parser.add_argument('-n', '--num-instruments', type=int, default=2, help='The number of stems.')
    parser.add_argument('-m', '--model', type=Path, default=ROOT / 'checkpoints' / '2stems' / 'model', help='The path to the model to use.')
    parser.add_argument('-o', '--output', type=Path, default=ROOT / 'output' / 'coreml', help='The output directory to place the model in')

    args = parser.parse_args()

    samplerate = 44100
    estimator = Estimator(num_instruments=args.num_instruments, checkpoint_path=args.model)
    estimator.eval()

    # Load wav audio
    wav, _ = librosa.load(ROOT / 'audio_example.mp3', mono=False, res_type='kaiser_fast', sr=samplerate)
    wav = torch.Tensor(wav)

    # Reproduce the STFT step (which we cannot convert to Core ML, unfortunately)
    _, stft_mag = estimator.compute_stft(wav)

    print('==> Tracing')
    traced_model = torch.jit.trace(estimator.separator, stft_mag)
    out = traced_model(stft_mag)

    print('==> Converting') # TODO: Dynamic input size?
    mlmodel = ct.convert(traced_model, convert_to='mlprogram', inputs=[ct.TensorType(shape=stft_mag.shape)])

    output: Path = args.output
    output.mkdir(parents=True, exist_ok=True)
    output_path = output / f'Spleeter-{args.num_instruments}stems.mlpackage'

    print(f'==> Writing to {output_path}')
    mlmodel.save(output_path)

if __name__ == '__main__':
    main()
